
\section{Introduction}
\label{sec:cilantro_intro}

% \rbcomment{Needs a nicer opening.}



The fundamental problem for any resource manager is to allocate a finite amount of
scarce resources to competing jobs.
% Allocating a finite amount of shared resources to competing jobs 
% is one of the most important tasks in cluster management.
% Allocating resources to competing jobs when there is contention for the finite shared resources
% in a cluster is one of the most important responsibilities of a cluster manager.
% Resource allocation is one of the most important responsibilities of a cluster manager.
% When allocating resources in a fixed cluster where there is contention for shared resources,
% among competing jobs or when autoscaling in
%  the elastic cloud where we need to keep costs at a minimum,
When doing so, we should ensure that the allocations fulfill the users' and the
organization's overall goals.
% 
% Traditionally, scheduling policies have optimized on various objectives,
Traditionally, resource allocation policies have aimed to
provide fairness~\citep{ghodsi2011dominant,demers1989analysis},
maximize resource utilization~\citep{gandiva},
maximize the amount of work done~\citep{ghodsi2011dominant},
or
minimize queue lengths~\citep{racksched,sparrow}.
% 
However,
these policies miss, or at best are imperfect proxies for what matters most to the users:
the performance of their jobs in terms of actionable real-world metrics
(e.g. P99 latency or throughput for a serving job).
Barring some recent exceptions~\citep{delimitrou2014quasar,chen2019parties,zhang2021sinan,jockey},
resource allocation systems have been largely oblivious to a job's real-world performance
(henceforth, simply \emph{performance}). 
% Such systems either require users to determine their resource requirements, or are typically based
% on heuristics such as resource utilization.


\insertResUtilIllus

To illustrate the pitfalls of performance-oblivious scheduling,
consider an example where two users are sharing a cluster of $100$ CPUs.
They are each serving database queries and care about their throughput:
the first user's service level objective (SLO) is 120 query-per-second (QPS),
while the second user's is 62 QPS.
If it was known that the resource-to-throughput curves of the two user's were as shown
in Figure~\ref{fig:toyexample},
a scheduler can allocate 40 CPUs to the first user and 60 to the second.
However, in practice, this resource to performance mapping is usually not available and other allocations do not work well.
For instance, a naive performance-oblivious fair scheduler which allocates 50-50, will not achieve the SLO of the second user.

% In this scenario, a performance-oblivious resource-based fair policy
% will equally allocate 50 CPUs to each user. \rbcomment{Show how 50-50 sharing is bad visually in Fig 1}
% However, suppose it was known that the resource-to-throughput curves of the two user's were as shown
% in Figure~\ref{fig:toyexample}.
% Hence, the first user can meet their SLO with 40 CPUs while the second can with 60.
% While a performance oblivious 50-50 fair allocation only satisfies the SLO of the first user,
% had we instead accounted for their performance and SLOs and
% allocated 40 CPUs to user 1 and 60 to user 2, they would both have achieved their SLOs.
% Even though this might seem unfair from a resource point of view, the first user may
% not complain since her SLO is satisfied.

% is resource-efficient, in that both jobs will be doing work,
% only the first user will achieve her SLO while the second one will not.
% Instead, if we
% allocate 40 CPUs to user 1 and 60 to user 2, they both achieve their SLOs.
% While this might seem unfair from a resource point of view, user 1 may not have complaints since
% her SLO is satisfied.


% policies are particularly pronounced when resources must be shared across applications in a multi-tenant environment.  For instance, consider the example in
% consider the example in
% Figure~\ref{fig:toyexample} where two users are sharing a cluster of 100 CPUs.
% They are running database queries from TPC-DS benchmark\cite{tpc-ds},
% and care about the throughput, i.e. the
% rate at which queries are processed. The jobs have different resource-to-throughput curves and service level objectives (SLO). The
% first user's SLO is 120 query-per-second (QPS) which requires 40 CPUs, and the second user's SLO is
% 62 QPS which requires 60 CPUs.
% % 
% While this is resource-efficient, in that both jobs will be doing work,
% only the first user will achieve her SLO while the second one will not.
% Instead, if we
% allocate 40 CPUs to user 1 and 60 to user 2, they both achieve their SLOs.
% While this might seem unfair from a resource point of view, user 1 may not have complaints since
% her SLO is satisfied.
% %In realistic situations, we often cannot satisfy the SLOs of all the jobs when there is a resources are oversubscribed and the scheduler should
% % decide how best to divide the resources among these competing jobs.
% %We assume the users are fully satisfied if they achieve their SLOs.
% % In this example, user 1 requires 30 CPUs to meet her SLO while user 2 requires 70.


% Despite being well studied~\citep{delimitrou2014quasar,delimitrou2013paragon,venkataraman2016ernest, morpheus, jockey, chen2019parties, zhang2021sinan}
% 
% Despite being well studied~\citep{delimitrou2014quasar,delimitrou2013paragon,venkataraman2016ernest, morpheus, jockey, chen2019parties, zhang2021sinan}

Despite a plethora of theoretical
work~\citep{demers1989analysis,ghodsi2011dominant,kaneko1979nash,kelly1998rate,gutman2012fair},
performance-aware scheduling has remained challenging to deploy in practice since the resource-to-performance mappings are not available.
%  studied~\citep{delimitrou2014quasar,delimitrou2013paragon,venkataraman2016ernest, morpheus, jockey, chen2019parties, zhang2021sinan}
% and having desirable outcomes,
% This is largely because of two reasons.
% Performance-aware allocation has two key challenges.
% First, it requires the scheduler to know the
% resource-to-performance mapping for each application. For instance,
% in the above example, we must know exactly how many resources
% are required to achieve the SLO of each user.
% Second, resources are often oversubscribed and we need to decide how best to share these
% resources among jobs which may or may not be interdependent.
% Depending on the use-case, we may wish to optimize for different criterion(s) which necessitates
% different allocation policies.
% Let us delve into these challenges in more detail:
% 
% \wg{Can be slightly rephrased as: However, performance-aware allocations for multiple resource types is difficult in practice due to the fact that computing such desirable allocations requires good knowledge of the allocation to performance mapping. That is, the scheduler must know exactly how many resources
% are required to achieve a given performance threshold. Such a mapping is hardly directly available in practical applications. While workloads can be profiled before execution to obtain this mapping, such profiling falls short on three aspects.  }
% 
% This is largely because such policies require
% knowledge of the \emph{resource-to-performance mappings} for each application.
% \kkcomment{psat work is profiling..}
% The resource to performance mappings are usually unknown a priori.
To address this, past work~\citep{venkataraman2016ernest,delimitrou2014quasar,zhang2021sinan}
 profile their workloads before execution to obtain
this mapping. Such profiling falls short on three aspects.
First, offline profiled resource-to-performance mappings
 may not reliably reflect a job's performance in an actual production
environment,
such as interference from other workers~\cite{delimitrou2013paragon}
and performance variability of servers~\cite{tailatscale}.
% where interference from other workloads~\cite{delimitrou2014quasar} can be non-trivial.
% For instance, multiple database servers running on the same node may contend for disk IO, degrading
% performance in unpredictable ways. 
Second, jobs' resource requirements change with time due to varying load (e.g. arrival rate
of external queries) and profiling typically cannot account for these
changes. 
%For instance, time-of-day and other factors influence the amount of queries that a database microservice must serve.
Finally, profiling the resource to performance mapping is burdensome 
for end users and additionally expensive for the organizations as 
it requires a large pool of resources to
exhaustively profile a wide range of resource allocations.
This is especially challenging when there is dependency between jobs,
such as in a microservices environment\cite{deathstarbench}.
%\emph{This motivates using an online learning mechanism which can continually learn and improve
%from real-time data.}



% Second, the choice of scheduling policy depends on the the chosen criteria.
Even if the resource-to-performance mappings are known,
the choice of scheduling policy depends on the the use case.
For instance, suppose in Fig.~\ref{fig:toyexample}, we wished to maximize the total throught 
of the cluster, instead of trying to satisfy each user's SLOs (i.e. users do not have SLOs).
In this case, we would allocate $\sim$$64$ CPUs to the first user and $\sim$$36$ to the second
% to achieve
for a total throughput of $\sim$$212$ QPS.
% we need to decide how best to divide these
% resources when all jobs cannot be fully satisfied simultaneously.
% More often than not there is a scarcity of resources and we need to decide how best to share these
% resources among competing jobs.
As more realistic examples,
in multi-tenant clusters, we may wish to use policies which balance
between performance and fairness~\cite{kelly1998rate,demers1989analysis,ghodsi2011dominant}.
% (e.g. Kelly
% mechanism~\cite{kelly1998rate}, DRF~\citep{demers1989analysis,ghodsi2011dominant}).
In contrast,
when we wish to provision resources to different microservices of the same application, we are
more interested in some end-to-end performance criterion,
such as application latency;
here, we may wish to allocate more resources to critical microservices which bottleneck the
performance.
Optimizing for such different criteria necessarily requires different allocation policies.
To support a diverse set of such criteria and
% \kkcomment{@Romil: fill in}
simplify the interface for the end users (i.e. organization, developers), ideally,
a scheduler should allow user to declare their desired allocation criteria, and then
optimize for these criteria by learning the resource-to-performance mappings.

% To fa

% Switching
% between global policies is challenging in many scheduling frameworks (e.g., doing so in Kubernetes
% requires installing 3rd party schedulers\cite{something}), and implementing new policies requires
% modifying the core scheduler itself.
% \rbcomment{I think I stretched this argument a little too much.} 
% Policies should optimize for the specific performance criteria
% \emph{This motivates the design of a modular platform which can operate
% atop the learned resource-to-performance mappings and facilitates
% a wide variety of policies which directly optimize for the desired criteria.}

% \emph{(ii)} Second, even when the
% \kkcomment{I like this paragraphs} \wg{+1! : )}

In this paper, we introduce Cilantro, a framework for allocating a single fungible resource type
(e.g. CPUs, containers) among competing jobs.
At the core of Cilantro is an online
learning mechanism which forms feedback loops with jobs
to allocate resources.
A pool of performance learners and load forecasters analyzes live feedback from jobs and
learns models to estimate
resource-performance curves and load shifts for each job.
% To support easy specification of performance-based policies, Cilantro introduces a
% performance-resource translation layer, which uses the learned models to convert the user's performance goals to resource requirements for the job and vice-versa. 
% 
% Currently,
Cilantro seamlessly enables the implementation of performance-aware policies in two settings:
\emph{(i)} multi-tenant resource allocation for independent jobs under different fairness criteria,
and
\emph{(ii)} resource allocation for constituent inter-dependent jobs (microservices) within an
application.
This is a marked departure from performance-oblivious resource-fair policies
which equally allocate resources, and those based on
unreliable proxy metrics such as CPU utilization and queue lengths.
Moreover, unlike heuristic-based policies
(using either surrogates~\citep{rzadca2020autopilot}
or performance metrics~\citep{chen2019parties,jockey})
which need to be tuned separately to optimize for
% desired performance criteria, Cilantro's modular
different allocation criteria, Cilantro's modular design automatically derives custom
policies which directly optimize for end-users' desired criteria. 
\rbcomment{Add connect to why online learning - it's practical because we have data from data monitoring tools and the action space is easily accessible (eg, we can quickly change resource allocations).}
% design supports a large class of allocation criteria.
% End-users can declare their desired criteria, and Cilantro automatically derives a custom policy
% to optimize for said criteria. 


% Moreover, traditional policies, even those accounting for
% performance~\citep{delimitrou2014quasar,chen2019parties,zhang2021sinan,jockey},
% are typically based on heuristics which are post-hoc evaluated on desired allocation
% criteria and then manually tuned to optimize for such criteria.
% This can be burdensome and unreliable, especially in the presence of load shifts and other changes
% in the environment.
% when there are changes in the environment.
% For


% The main strength of Cilantro lies in the online nature of its learning mechanism.
% First, online learning
% methods are not bound by the amount of data used for learning - they can continually keep learning
% and improve over time. Second, since the learners learn from runtime data generated from live
% workloads, they can reliably account for real-world phenomenon, such as performance variability of servers \cite{tailatscale}.
% Third, online
% learning reduces the burden on users since they are no longer required to manually estimate the
% resource demand and any updates to the workload are automatically accounted for by the learner.
% \wg{Maybe this para can be merged with the one above to emphasize the importance of online learning together}

%
\rbcomment{We need to zoom in on the challenges. Create a table to compare against related work.}
\rbcomment{Much simpler version of the architecture in the intro - learn the mapping, use the scheduler to allocate resources.}
Our proposed solution solves three key technical challenges.
% overcome. \wg{Can slightly rephrase: Although the online learning block within Cilantro is effective
% and important, there are two key challenges that need to be overcome.}
First, due to the lack of
sufficient data, online learning models have a high degree of uncertainty in the early stages.
To operate without accurate estimates of resource-to-performance curves, Cilantro
informs scheduling policies with confidence intervals of its estimates.
Policies are designed using the optimism in the face of uncertainty (OFU)
principle~\citep{bubeck2012regret},
% \rbcomment{Should we mention OFU here?}This enables policies to
which accounts for this uncertainty when making allocation decisions
until the estimates become more accurate.
% conservative but best-effort resource allocation decisions until the estimates become more accurate.
% conservative but best-effort resource allocation decisions until the estimates become more accurate.
Second, to support a diversity of resource allocation criteria,
Cilantro's design is modular where
estimation of resource-to-performance mappings and forecasting of load shifts is decoupled from the
allocation policies itself.
% is: Asynchronous is not clear
Third,
to avoid unnecessary bottlenecks, Cilantro adopts an event-triggered asynchronous design.
This is crucial since
Cilantro's different components may operate at different frequencies:
different applications may emit their performance metrics at irregular intervals,
the time for updating a learning model depends on the model itself,
and the frequency of changing an allocation may depend on the agility of the cluster's resource
manager.

% Second, extracting and communicating performance metrics are not always feasible for applications,
% since they can be deeply embedded in the application logic. Cilantro includes a side-car client which
% embeds with applications and provides a uniform interface to extract performance metrics. In case
% performance metrics are unavailable, Cilantro also provides non-invasive performance measurements of
% applications to estimate the resource requirements.
% \rbcomment{This needs to change to talk about PRT}
% \kkcomment{we should back this last sentence up. Is it better to talk about this later?.}
% Third,
% \kkcomment{last sentence is unclear. are we just saying we can use a proxy metric or that users can
% specify a default}.

We have implemented Cilantro as an open-source extension to the Kubernetes core scheduler, available at \href{https://github.com}{\textcolor{blue}{(anonymized)}}. To evaluate Cilantro, first we deploy it on a 1000-CPU multi-tenant cluster which includes a diversity of real-world, latency and throughput-sensitive jobs.
% , such as database query serving, prediction training, and machine learning (ML)
% inference.
On three different allocation criteria, Cilantro's policies are able to outperform 9 other
baselines, and is able
to compete with oracular policies
which know the resource-to-performance mappings a priori on resource
efficiency, fairness, and strategy-proofness.
When compared to resource-fair allocation, it is able to 
increase the
performance of $1/3$ of users in the clusters by $1.2-3.7\times$.
Second, we evaluate Cilantro on a 160 CPU cluster where we wish to
allocate CPUs to constituent microservices of an 
application. Here, Cilantro is able to minimize the end-to-end P99
latency of the application by
$82\%$ when compared to a resource-fair scheduler and by
$43\%$ when compared to the next-best performance-aware baseline.
% without requiring any information about the data flow between microservices.
% \rbcomment{I think this is a cool point that we need to highlight more}
% \rbcomment{Microbenchmarks?}

% Moreover, we extend Cilantro to
% support performance-based autoscaling policies, outperforming baselines to satisfy a given
% performance target.
% Finally, we demonstrate some properties of performance-aware scheduling policies
% enabled by Cilantro, such as strategy-proofness, which incentivizes users to state their performance
% objectives truthfully.


