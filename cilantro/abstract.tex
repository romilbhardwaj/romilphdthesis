Going a layer above in the ML stack, we now look at the cluster management layer where resources are shared between multiple ML applications. Here, we observe that 
% \vspace{-0.05in}
% a data centers or elastic cloud.
% Historically, fairness metrics
traditional systems for allocating finite cluster resources among competing jobs
%  in multi-tenant environments have either
have either aimed at providing fairness,
relied on users to specify their resource requirements,
or have estimated these requirements via surrogate metrics (e.g., CPU utilization).
These approaches do not account for a job's real world performance (e.g. P95 latency), and as a result produce inefficient resource allocations.
% Existing performance-aware systems use offline profiled data and/or
% are designed for specific allocation objectives.
% These approaches fall short on what a user cares most: how well their job performs in
% the real world (e.g. P95 latency).
% % This emphasis on resource-fairness neglects the metric
% % that is most important to users - the performance of their jobs.

In this chapter, 
we argue that resource allocation systems should directly account for real-world
performance and the varied allocation objectives of users.
In this pursuit, we build \cilantro{}. %a framework for performance-aware resource allocation.
% , a framework for performance-aware resource allocation.
% metrics

At the core of \cilantro~is an online learning mechanism which forms feedback loops with the jobs
to estimate the resource to performance mappings and load shifts.
% observe its and appropriately change the allocation.
% Not only does
This relieves users from the onerous task of job profiling
and collects reliable real-time feedback.
This is then used to achieve a variety of
user-specified scheduling objectives.
Cilantro handles the uncertainty in the learned models by adapting the
underlying policy to work with confidence bounds. 
% By translating performance requirements to resource demands, \cilantro{} can adopt a wide variety of performance-aware scheduling policies.
% Cilantro's modular design enables users to declare their desired allocation criteria, and then derive custom algorithms designed to optimize for these criteria.
We demonstrate this in two settings.
First, in a
multi-tenant 1000 CPU cluster with 20 independent jobs,
three of Cilantro's policies outperform 9 other baselines on
three different performance-aware scheduling objectives,
improving user utilities by up to $1.2-3.7\times$ \cameratext{and performs comparably to oracular policies}.
Second, in a microservices setting, where 160 CPUs must be distributed between 19
inter-dependent microservices, Cilantro outperforms 3 other baselines,
reducing the end-to-end P99 latency to  $\times 0.57$ the next best baseline.
% When compared to a performance oblivious fair sharing policy, Cilantro improves
% the performance for a third of the users
% by $1.2-3.7\times$ in the first setting, and reduces the end-to-end P99 latency by upto $82\%$
% in the second. 
% 
% We find that online learning policies implemented on
% \cilantro{} outperform nine baselines by $1.5-4\times$ on three different performance-aware
% evaluation criteria.
% Next we consider allocating a finite amount of resources to different microservices
%  within an application to minimize its average end-to-end latency and find that Cilantro outperforms
% three other baselines by $XX-XX\times$.
% In an autoscaling experiment, \cilantro's policy outperforms three baselines by satisfying a target
% performance requirement while minimizing the cost.

% \kkcomment{add generality, change title}
