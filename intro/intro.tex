\chapter{Introduction}
\label{ch_intro}

% Define fillme
\newcommand{\fillme}{\textcolor{red}{FILL ME}}

Machine Learning is emerging as a major driver of economic value today. Various estimates \romil{Add economic impact here.}

% ML needs data and compute
Machine learning workloads are very resource intensive. The process to train these models requires multiple iterations over large amounts of data to generate progressively better models. Naturally, training these models requires a large amount of compute. Figure \ref{fig:compute_demand} shows the compute demand of machine learning models over time. For comparison, training Llama 2 in 2023 required \fillme{} FLOPs, which is \fillme{} times more than the largest model of 2012, AlexNet. As a result, \romil{AddMore}

% ML has large demand
On the other hand, the supply of compute is struggling to keep up with the demand. Moore's law, which postulates that the number of transistors on a chip doubles every two years, is hitting a wall. As transistors become smaller, reaching the size of a few nanometers, physical limitations such as quantum effects and heat dissipation become significant challenges. This makes it increasingly difficult to continue shrinking components at the pace predicted by Moore's Law. Worse yet, Dennard scaling, which refers to the shrinking of transistors while maintaining constant power density, has ended because of increased leakage currents and heat dissipation issues at smaller transistor sizes. Combined, these factors have caused a decline in the rate of growth of compute resources.

The growing compute needs of Machine Learning and the shrinking supply of compute for these models has created a supply-demand gap. This gap manifests in two forms - lack of resource availability and high costs of training ML models. \romil{AddMore}
% Availability - use the news article links
% Cost - quote from https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems

% Bridging the gap by making ML more efficient - why we do so.
In this thesis try to bridge the compute supply-demand gap by explore techniques to make machine learning more resource efficient. By doing more with less, we can reduce the demand for compute resources. \romil{AddMore}

Looking at the machine learning stack allows us to see where we can improve efficiency. The machine learning stack is composed of three layers.

\begin{enumerate}
    \item Application layer - This layer is composed of the machine learning models and the algorithms used to train them. Machine Learning practicioners use tools and frameworks such as PyTorch, TensorFlow, Airflow to define their models and specify their workflows for training them.
    \item Cluster Manager - This layer is responsible for managing the cluster of machines that are used to train the machine learning models. It is responsible for scheduling jobs, allocating resources to them and monitoring their progress. Examples of cluster managers include Kubernetes, Mesos, YARN.
    \item Specialized accelerators - This layer is composed of specialized hardware that is used to accelerate the training of machine learning models. At this layer, a lower level scheduler decides the order of execution of jobs on the physical node. Examples of specialized accelerators include GPUs, TPUs, FPGAs.
\end{enumerate}

% Ekya - Specialized accelerators
This thesis builds techniques to improve ML efficiency by rethinking how our resource allocation systems work. These techniques span all three layers of the machine learning stack.

At the accelerator layer, we build Ekya to make continuous learning, a compute-intensive but necessary technique to run high-accuracy inference, 4x more efficient. These eficiency improvements are enabled by the Thief Scheduling algorithm in Ekya, which inverts the work-stealing idea \cite{something} to allow individual jobs to "steal" resources from one another. In doing so, the scheduler reallocates resources from less critical inference jobs to more impactful retraining jobs, prioritizing configurations that maximize accuracy improvements relative to resource costs. Complementing this, the Microprofiler in Ekya accurately assesses the resource demands and potential accuracy gains of various retraining configurations. This profiling enables the Thief Scheduler to make informed and efficient decisions about resource distribution. Together, the Thief Scheduler and the Microprofiler allow Ekya to effectively manage resource constraints and address data drift in edge environments, achieving the same accuracy while using 4x fewer resources.

% Cilantro - Cluster manager
At the cluster manager layer, we find that the current resource allocation model used today is inefficient. Most cluster managers are unaware of a job's performance for a given resource allocation, and instead expect users to state their resource requests when submitting jobs. However, because these requests are often best-effort estimates made by humans, they are inaccurate and lead to inefficient resource allocation. To address this, we build Cilantro, which uses an online learning mechanism to form feedback loops with jobs, thereby estimating resource-to-performance mappings and adapting to load shifts. This approach alleviates the need for job profiling and allows for a variety of user-Â defined scheduling objectives. Cilantro's policies are uncertainty-aware, adapting to the learned models' confidence bounds. It demonstrates effectiveness in two scenarios: a multi-tenant 1000 CPU cluster, where it outperforms baselines and improves user utilities up to 1.2-3.7x, and a microservices setting, where it reduces end-to-end P99 latency significantly compared to baselines. In doing so, Cilantro marks a significant departure from traditional performance-oblivious policies, offering a more flexible, accurate, and efficient method of resource allocation.

% Escher - Application layer
Finally at the application layer, we discover that modern machine learning applications have very diverse scheduling requirements and their performance is highly sensitive to the underlying cluster manager's scheduling decisions.  As these applications evolve, they require \textit{scheduling flexibility} which allows them to exercise control over how their tasks are placed and executed. However, in today's systems, this \textit{scheduling flexibility} comes at the cost of simplicity. Expressing a custom scheduling need requires an application to modify the underlying monolithic scheduler in the cluster manager, a task which is inherently complex and challenging to maintain. In this thesis, we introduce ESCHER, a cluster scheduler design that allows machine learning applications to express their scheduling requirements without the complexity of reimplementing a cluster scheduler. ESCHER introduces ephemeral resources, an abstraction which allows applications to express scheduling constraints as resource requirements. These requirements are then matched to available resources through a simple mechanism. Implemented on Kubernetes and Ray, ESCHER demonstrates the ability to express common policies found in monolithic schedulers, while also providing the flexibility for applications to easily create custom policies that were previously unsupported. % Add more results

% Write some conclusion
\romil{AddMore}


