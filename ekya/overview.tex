\section{Overview of {\name}}
\label{sec:overview}

The primary purpose of {\name} is to help network administrators investigate reports of poor network performance, i.e., RTTs between the clients and the cloud locations being above the desired SLOs. {\name} attributes such {\em path-level} latency degradations to localize the faults at the {\em AS-granularity}. %An approach that we considered was to use network tomography to infer AS-level latencies from the path-level latencies, and find the faulty AS based on deviations from their normal latencies. However, even though network tomography is a well-studied field~\cite{tomography papers}, no practical solutions exist especially at the scale of the Internet. This is due to two main reasons: ($i$) the passively collected measurements have a highly skewed visibility of the paths between the {\azure} locations and the different client AS'es (\S\ref{section-2}), and ($ii$) the sheer scale of the potential number of bad AS-paths preclude a full-scale probing because of the substantial overheads (Figure~\ref{??}). 
%{\name}'s design uses the passively collected RTT measurements to get a coarse localization of the problem (\xref{sec:passive}), and then use as few active probes as possible to finely diagnose those latency degradations that affect the most users (\xref{sec:active}). 
%Thus, in our design on {\name}, we strive to accurately diagnose those latency degradations that affect the most users users but using as few active probes as possible. 
In this section, we explain the high-level intuitions behind our solution before elaborating on the details in \xref{sec:passive} and \xref{sec:active}. 

%In designing {\name}, thus, we have two high-level goals that are important in practice: ($a$) accurately diagnose those latency degradations that affect the most users users, and ($b$) use as few active probes as possible. {\name} meets the goals with two core techniques both built upon our operational experience. Here we first elaborate their intuitions before presenting the details in next few sections.

\subsection{Two-level blame assignment}
\label{subsec:two-level}

Modeling the paths between the cloud locations and client at the AS-granularity, as per traditional approaches \cite{castro2004network, zhang2004planetseer}, is problematic. First, the coverage of the measurements is skewed and therefore there are often not sufficient measurements to identify the contributions by each AS in the path. Second, modeling the graph at the AS granularity introduces ambiguities, e.g., a large AS like Comcast might have a problem along certain paths but not all. The impact of both these problems is compounded when paths in the Internet change.

Instead of modeling a path as a concatenation of AS'es, {\name} views each path in two granularities---a {\em coarse-grained} segmentation of the path into three segments of ``client'' (the client AS), ``cloud'' (the cloud AS), and ``middle'' (all AS'es between the cloud and the client), and then a {\em fine-grained} AS-level view  of only the ``middle'' segment. Correspondingly, {\name} localizes the fault in two steps. 
\begin{enumerate}
    \item Attribute the blame for latency degradation on one of the three coarse segments, using {\em only} ``passive'' data.
    \item If a fault is attributed to a middle segment, {\name} (optionally) moves to the ``active'' phase to trigger probes for AS-level fault localization. 
\end{enumerate} 
Note that modeling {\em each} path into three segments (albeit at a coarse granularity) allows {\name} to identify localized issues in an AS that occur only in that path (but not elsewhere in that AS, thus avoiding the ambiguities explained above). 

%The coarse-grained blame assignment at the segment level is inspired by the operational observation that most cases of inflated latency is typically attributable to a single segment among the ``client'', ``middle'' and ``cloud'' along the end-to-end path (\ga{Support analysis?}). For example, inflated latency between an {\azure} location and the client might be due to the middle segment or the client prefix but not due to moderate increases in both. Inspired by this observation, we propose a simple-yet-practical algorithm to uniquely attribute a fault to one bad segment. %We notice that the two-level blame assignment over a network topology  
%We notice that the coarse-grained blame assignment of {\name} can be viewed as a practical instantiation of network tomography over potentially correlated links (e.g.,~\cite{imc10}). \ga{Correlated? What does this mean?}
%though we do not make much idealized assumption as in boolean tomography.

\noindent{\bf Coarse Segmentation:} The three-way segmentation of each path into client, middle, and cloud segments is borne out of {\azure}'s operational requirements. Localizing the fault to the cloud or the client AS is already actionable. A {\em cloud-induced} latency degradation would directly lead to personnel being deployed for further investigation. When the problem lies with the {\em client} network (e.g., the client's ISP), the cloud operator typically cannot fix the problem other than informing the client about their ISP choices (e.g., \cite{youtube-interruptions}). Thus, our segmentation proved to be pragmatically beneficial and allowed for a phased deployment. Our coarse-grained fault localization solution based on passive measurements has been deployed in production since Nov 2017.  %While we considered other sophisticated ways to cluster the AS'es to produce a coarser graph, the simple segmentation along client, middle, and cloud segments proved to be a pragmatically beneficial choice.

%The three-way segmentation of each path into client, middle, and cloud segments is largely aligned with the types of remedial actions that follow the blame assignment. A {\em cloud-induced} latency degradation would fall under the direct control of cloud operators leading to tools and personnel being deployed for further investigation. When the problem lies with the {\em client} network (e.g., the client's ISP), the cloud operator typically cannot fix the problem other than informing the client about their ISP choices (e.g., \cite{youtube-interruptions}). If the problem is caused by any of the AS'es in the {\em middle}, then the mitigation techniques include switching egress peering points and communicating with the administrators of the problematic AS'es. (\ga{Then why do you want to go to AS-level?}) Thus, the coarse-grained localization already provides actionable information to network engineers on the nature of the problem, even before the fine-grained localization.
% More importantly, assigning the latency inflation at the granularity of ``cloud'', ``middle'', and ``client'' sometimes already provides actionable information to help engineers take remedial steps.
% Note that even if there are multiple bad segments, {\name} will always return one of them, i.e., no false positives.


\subsection{Impact-proportional budgeted probing}
\label{subsec:impact-proportional}

%{\name}'s characterization of the fault into cloud, middle, client segments is incomplete towards the goal of AS-level fault diagnosis as the ``middle'' segment may contain multiple AS'es. However, as explained earlier, the passive measurements are insufficient to do AS-level fault diagnosis because of the lack of coverage of all the paths (see \xref{section2} and \S\ref{subsec:problem}). 
%The two-level blame assignment is incomplete, as it is insufficient to only use the passive end-to-end measurements to uniquely localize a middle-segment fault to a specific link. This is because to distinguish two potential fault links, there must be a measurement path traversing one and only one of them~\cite{imc14}. Cloud providers often lack such measurement diversity in the middle segment.  Clients are more likely to be mapped to a single ``nearest'' cloud site than others, so it is hard to localize the faults near the clients as these downstream links always co-occur with the same upstream links (more details in \S\ref{subsec:formulation}).

%{\name} uses active probes (traceroutes) to compensate for the lack of path coverage. Since probing all the paths for full coverage is prohibitively expensive given the sheer number of paths, {\name} sets a {\em budget} on the active probes and allocates the probes to bad middle segments based on their expected {\em impact} on clients. For each bad middle segment, {\name} estimates {\em how many clients will be affected} and {\em for how long}, and prioritizes those that are expected to affect more clients for longer time. 
{\name} uses active probes (traceroutes) only for fine-grained localization of ``middle'' segment blames, as these segments may contain multiple AS'es. Since probing all the paths for full coverage is prohibitive given the sheer number of paths (172 million per day; \xref{subsec:active-eval}), {\name} sets a {\em budget} on the active probes and allocates the budget to probing bad middle segments based on their expected {\em impact} on clients (\xref{subsec:ip-vs-impact}), i.e., by estimating {\em how many clients will be affected} and {\em for how long}. %, and prioritizes those that are expected to affect more clients for longer time. 
{\name}'s budgeted probing is unique in two aspects compared to prior work (e.g.,~\cite{zhang2004planetseer, 35590, madhyastha2006iplane}).

Spatially, {\name} defines the importance of a segment in proportion to its impact on actual {\em number of clients}, rather than just the addresses in the IP block \cite{35590}. In {\azure}'s operations, large IP address blocks often have fewer active clients than smaller IP blocks, thus making it desirable to predict and prioritize issues affecting the most active clients.
%While the size of an IP block can sometimes indicate the number of users, in our context, large IP blocks may have proportionally less active clients of the cloud service than smaller IP blocks. 

Temporally, %the number of active clients in a middle segment does not necessarily correlate with the {\em timespan} for which latency degrades. By 
by {\em estimating} the timespan of each latency degradation, {\name} can focus on long-lived problems rather than fleeting problems that may end shortly after we probe it, thus wasting our limited budget for probing. Note that we do not need very precise estimate on the timespan of a problem because of the long tail distribution of problem durations (\xref{subsec:long-tail}). It would suffice if we only separate the few long-lived problems from the many short-lived problems.%, and as shown in \S??, this can be estimated by how long the problem has lasted thus far. 

\subsection{End-to-end workflow} 

{\name}'s workflow is as follows. RTT data is passively collected at the different cloud locations and sent to a central data analytics cluster. %A data analytics job is periodically scheduled to run to categorize all RTTs that are higher than the desired SLOs and assign blames at coarse-grained segments; \xref{sec:passive}. 
A data analytics job periodically makes coarse-grained blame assignments to all bad RTT instances; \xref{sec:passive}. 
These blame assignments trigger two sets of actions. The middle segment issues are ordered based on their impact and active probes are issued from the appropriate cloud servers for finer localization; \xref{sec:active}. All the latency inflations are prioritized based on their impact, and the top few are sent to network administrators for investigation (details in \S\ref{subsec:implementation}). 

%\junchen{%below is from the old 4.1. needs to be merged in sec 3 as current operational practices:
%The primary purpose of {\name} is to help network administrators investigate reports of poor network performance, specifically, the RTTs between the clients and the cloud locations that are above the desired SLOs. A key step in their investigations is identifying the {\em location} (at the granularity of an AS) that is the cause of the increased RTT. As described in \xref{sec:overview}, the result of the fault localization helps operators take a variety of different mitigating steps.
%\ga{Stitch into Intro.} Our conversations with the administrators of \azure revealed that, in the absence of problem localization, their current practice is to pick a handful of incidents of poor network performance from the last {\em day} (or from the last few hours), often in an ad hoc manner, and investigate these. This often results in investigating issues that are not directly fixable by the cloud operators (e.g., client ISP fault) as well as ignoring issues that are severe (e.g., cloud's fault with many affected clients). Moreover, this manual process tends to be slow, so much so that many faults get ``auto-resolved'' by the time the administrators get around to investigating them. Thus, the need for prioritizing incidents and investigating them closer to their occurrence (i.e., near real-time). }