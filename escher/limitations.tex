\section{Discussion}
\label{discussion}

\noindent\textbf{Why use ESCHER?}
%For common scheduling policies, ESCHER's primary benefit compared to a monolithic cluster manager is not performance.
%Rather, its benefit is making it easy for an application to specify and implement unsupported policies without requiring any changes to the cluster scheduler. This is essential for developing new applications with sophisticated scheduling constraints that are not yet supported by the underlying scheduler.
For common scheduling policies, ESCHERâ€™s primary benefit compared to a monolithic cluster manager is not performance. Rather, it's the ease to specify and implement new policies without requiring any changes to the cluster scheduler. This unlocks developing new applications with sophisticated scheduling constraints that are not yet supported by the underlying scheduler.
One example is the \emph{composition} of affinity and gang scheduling policies used in the distributed training example in \Cref{sec:eval:tune}, which is not supported by Ray's native scheduling primitives. Another example is DAG-based scheduling, which is offered natively by Ray but not Kubernetes.
DAG-based scheduling can be implemented by leveraging the task signaling primitive~(\Cref{sec:sched:primitive}). Effectively, ESCHER expands the set of policies a monolithic cluster manager can support.

%  For instance, the composition of affinity and gang scheduling policies in the EscherTune application was trivially achieved by concatenating gang scheduling and affinity ephemeral resource requirements. Implementing the same policy without ESCHER is possible, but would have required a deep understanding of the ray scheduler source code and would be a significantly larger effort to develop a new scheduling API. Similarly, DAG scheduling can expressed in ESCHER as a simple composition of the Task Signaling primitive defined in Section \ref{sec:sched:primitive}. Applying DAG scheduling otherwise requires integrating a completely different system, such as Apache Airflow\cite{airflow}.

Two-level schedulers~\cite{omega, mesos} achieve the same goal by exporting scheduling control directly to applications, but in doing so they also require applications to implement the entire scheduler themselves~(Section \ref{sec:arch:evolve}). ESCHER on the other hand requires minimal changes to application code: it required adding only two lines of code to MapReduce, five lines to AlphaZero and fifty lines to EscherTune, each of which had widely varying policy requirements. For policy composition especially, this ease of development is due in part to the use of ESLs.

% While the performance of ESCHER is often comparable to a hardcoded policy in the cluster scheduler, the primary value-add of ESCHER is that is allows the application to specify a policy without requiring any changes to the underlying framework scheduler. We note that two-level schedulers \cite{omega, mesos} also achieve the same goal by exporting scheduling control directly to applications, but in doing so they also require applications to implement a scheduler which must reason about tasks and resources in a distributed environment (Section \ref{sec:arch:evolve}). ESCHER on the other hand requires minimal changes to application code - it required adding only two lines of code to MapReduce, five lines to AlphaZero and fifty lines to EscherTune, each of which had widely varying policy requirements. 

% Additionally, some policies are very naturally expressed in ESCHER. For instance, the composition of affinity and gang scheduling policies in the EscherTune application was trivially achieved by concatenating gang scheduling and affinity ephemeral resource requirements. Implementing the same policy without ESCHER is possible, but would have required a deep understanding of the ray scheduler source code and would be a significantly larger effort to develop a new scheduling API. Similarly, DAG scheduling can expressed in ESCHER as a simple composition of the Task Signaling primitive defined in Section \ref{sec:sched:primitive}. Applying DAG scheduling otherwise requires integrating a completely different system, such as Apache Airflow\cite{airflow}.

% ESCHER on the other hand requires applications to only specify the policy as ephemeral resource requirements which requires minimal changes to existing code - it required adding only two lines of code to MapReduce, five lines to AlphaZero and fifty lines to EscherTune, each of which had widely varying policy requirements. 
%\noindent\textbf{Limitations and future work.} 
\noindent\textbf{Limitations.} 
A key goal of ephemeral resources is to provide a simple and narrow API that is easy to implement by most cluster managers. As a result, ESCHER eschews abstractions that would require complex implementations such as transaction support~\cite{omega} (which would allow to trivially implement gang scheduling) or utilization-based scheduling, such as load balancing~(\Cref{sec:softconstraints}).     
%This decision  Thus, ESCHER does not allow transactions on resource availability, such as for gang scheduling or resource utilization-based policies~(\Cref{sec:softconstraints}). 
While the application can still implement these policies using ghost tasks, these implementations have inherently a higher overhead. 
%This affects performance, as developers need to implement such functionality at the application level which is inherently slower and sometimes more cumbersome. 
%when compared to implementing the policy in the scheduler or providing transaction semantics to applications like Omega\cite{omega} does.
% Only after submitting a task can an application know if sufficient resources are available to execute the task.
%However, as we show in \Cref{sec:eval:gangscheduling}, the application can still use ghost tasks to implement such scheduling policies with relatively low overhead.
Of course, if applications require higher scheduling performance, we can eventually implement these policies in the cluster manager. Even in this case, ESCHER remains valuable as it can bridge the gap by enabling the applications to implement these policies before the cluster manager does.

%Meanwhile, the framework developer is still free to eventually integrate such policies into the core scheduler, e.g., when user demand for high performance increases, alongside the ephemeral resources abstraction.

Another limitation of ESCHER is that it doesn't expose the resource availability to the application, which means that the only way for an application to learn that there are not enough resources available is by submitting a task which hangs. As a result, the application might have to explicitly kill the tasks that hang, adding to the complexity. We do not expose resource availability is because it would not fully solve the problem: there is still a race condition when two tasks on different machines simultaneously request more than the available resources (e.g., a single GPU is available and two tasks simultaneously request one GPU each). This problem is exacerbated by the fact that ephemeral resources can be dynamically created, modified, or destroyed. One solution to this problem is providing transaction semantics, which, as mentioned above, ESCHER eschews due to its complex implementation. An avenue for future work would be to alleviate the challenges of handling such a dynamic environment, e.g., by using lazy execution or extending the API to allow constraints on ephemeral resources.

%Another fundamental property of ESCHER is that ephemeral resources are created, destroyed, and requested dynamically. Only after submitting a task can an application know if sufficient resources are available to execute the task, and even then the required resources may be created or destroyed asynchronously.
%Analogous to the differences between interpreted and compiled languages, the dynamic nature of ESCHER grants high development velocity by supporting a wide range of policies.
%However, since the result of the API calls depends on run-time information, it can also be more difficult to prove correctness and debug.
%Future work could explore ways to guarantee correctness in this completely dynamic setting, e.g., through lazy execution or by placing constraints on the ephemeral resources API.

%  This dynamic nature grants ESCHER its flexibility in supporting a wide range of policies. However, since the result of the API calls is dependent on the state of the cluster, it can be non-trivial to prove correctness of policies implemented in ESCHER. In contrast, schedulers using a declarative language for policy specification (SQL in DCM \cite{dcm-osdi20}, STRL in Tetrisched\cite{tetrisched}) can employ static checking to verify correctness of policies. Future work can explore how a static policy plan can be generated from a dynamic specification in ESCHER for verifying correctness. \romil{We need better future work here}

% A fundamental difference between ESCHER and other schedulers is that applications in ESCHER cannot perform transactions on resource availability, such as for placing reservations.
% Only after submitting a task can an application know if sufficient resources are available to execute the task. An ESCHER application must use mechanisms such as ghost tasks to perform incremental locking on resources, such as for gang scheduling. It also complicates (but does not preclude) the implementation of resource utilization based policies, such as the objective functions in \cref{sec:softconstraints}.

% Not providing transactions on resources is a deliberate design choice made in ESCHER. First, scheduling logic in an application gets significantly more complex if it must operate under transaction semantics. For instance, applications running on Mesos\cite{mesos} need to implement a local resource manager for book keeping of the offered resources and the tasks they are assigned to. Second, we discover that ghost tasks in ESCHER achieve the same goal through incremental locking without requiring any concurrency control support from the framework scheduler, such as in Omega\cite{omega}. As shown in \cref{sec:eval:gangscheduling}, ghost tasks introduce an overhead but significantly reduce the implementation complexity of both the application and the scheduler. This is also an opportunity for future work, where the framework can offer a dynamic choice between transactions for performance-sensitive applications, while allowing ghost tasks for developer agility.
%\swang{Not sure if I understand what "hybrid of transactions" means?}


% Can also start like - ESCHER's flexibility stems from its dynamic nature..  or, Unlike other static 
% Another fundamental property of ESCHER is that policies are defined and executed dynamically. Policy specification is done by invoking the \lstinline{set_resource} API at runtime and the policy is executed by requesting ephemeral resources at task submission. This dynamic nature grants ESCHER its flexibility in supporting a wide range of policies. However, since the result of the API calls is dependent on the state of the cluster, it can be non-trivial to prove correctness of policies implemented in ESCHER. In contrast, schedulers using a declarative language for policy specification (SQL in DCM \cite{dcm-osdi20}, STRL in Tetrisched\cite{tetrisched}) can employ static checking to verify correctness of policies. Future work can explore how a static policy plan can be generated from a dynamic specification in ESCHER for verifying correctness. \romil{We need better future work here}

% \swang{I like the above points about ghost tasks (as long as we're not duplicating them elsewhere, but it'd be good to mention at least one other limitation. Otherwise it will come off as not very enlightening for a discussion section.}

