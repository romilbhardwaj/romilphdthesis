\section{Motivation}
\label{sec:escher_motivation}

% Gandiva. Problems - conflicting policies, elastic scaling to more nodes, dynamic scheduling policy when jobs change, composable policies
%\name{} is motivated by the challenges faced by applications in exercising fine-grained control over scheduling of compute resources  in a distributed environment. Consider Gandiva\cite{gandiva}, a system for distributed training of machine learning jobs which builds on Kubernetes \cite{kubernetes} for task orchestration. Gandiva applies compute and data co-locality and load-balancing through mechanisms such as time-sharing and task migration to speed up hyperparameter search in machine learning experiments. Co-locality avoids network overheads between jobs running across GPUs, while load-balancing ensures the workload is evenly split across the cluster to avoid network and compute bottlenecks. Additionally, some model-parallel jobs may require all-or-none scheduling semantics \cite{shivaram-gpustudy}, where all task instances requested by the job must run concurrently for it to progress.

\name{} is motivated by the need of many modern applications to have fine-grained scheduling control in a distributed environment. Consider Gandiva\cite{gandiva}, a system for distributed training of neural networks that builds on top of Kubernetes\cite{kubernetes}. Gandiva has a diverse set of scheduling requirements, including co-locality, load-balancing, and gang scheduling. In particular, Gandiva uses co-location of data and computation to avoid unnecessary data transfers, load-balancing to split the work evenly across the cluster to avoid stragglers, and gang-scheduling to ensure that the training job makes progress. 

Gandiva's scheduling requirements exposes two challenges. First, some of these requirements are conflicting. On one hand, Gondiva wants to  co-locate some tasks on the same node; on the other hand, it wants to evenly distribute (i.e., load-balancing) tasks across the cluster. Second, Gandiva's scheduling policy can be naturally expressed as an hierarchical composition of simple policies: gang schedule some sets of tasks, where these sets are load balanced across the cluster, and where tasks in the same set are co-located on the same machine. Unfortunately, expressing this composition is challenging both for the application and the underlying execution framework: the framework must be general enough to support arbitrary scheduling policies, and the application must be able to express these requirements, as well as resolve the conflicts between these requirements. %todo: Highlight that only applications know how to resolve conflicts.

%These scheduling requirements have two notable characteristics, which can be observed generally across modern distributed applications. First, the application has seemingly divergent scheduling requirements - for instance, some sets of Gandiva tasks need to be co-located on the same node, while others need to be evenly spread across the cluster. Second, the overall scheduling policy of the application can be expressed as a hierarchical composition of simpler policies. In Gandiva, the scheduling policy is a composition of task co-location, followed by load-balancing and finally gang-scheduling applied to tasks belonging to the same job. These scheduling requirements present challenges for both, the application and the underlying execution framework - applications must be able to express their specific scheduling requirements, while frameworks must be general enough to support arbitrary scheduling policies. %todo: Highlight that only applications know how to resolve conflicts.
 
% Explore existing solution space
% 1) Write custom scheduler
For frameworks which offer only a fixed set of scheduling policies to choose from, the approach to expressing custom policies is to modify the core scheduler in the framework. However, this is undesirable for multiple reasons. First, modifying existing scheduler code can be arduous and requires a deep understanding of the working of the framework. Second, any changes in the core of the framework may introduce bugs. Finally, it creates a strong dependence between the application and the framework, limiting the portability to other frameworks.

% 2) Expose Mechanisms to Control
% 2a) Direct scheduler API
Many frameworks recognize the need to extend scheduling control to applications and offer mechanisms to support heterogenous application-level scheduling objectives. Some frameworks \cite{kubernetes} expose control in the form of REST APIs, where applications can query the list of waiting tasks and specify the precise node where the task must be scheduled. While this offers fine grained control over scheduling and portability, this approach has several drawbacks. First, this introduces a single point for failure for the entire scheduling subsystem. Frameworks may want to employ distributed schedulers for performance and robustness, but this approach reverts them to a centralized scheduler residing in the application. Second, it promotes a tight coupling between tasks and physical nodes. This is undesirable when nodes can fail or are transient in nature, like in a serverless computing environment, making fault-recovery difficult. Third, the application is burdened with implementing a scheduler whereas it only requires the ability to specify policies at a high level.

% 2b) Resource management API
Some frameworks \cite{yarn, mesos} assume that applications' scheduling objectives are strongly tied with allocation of physical resources. Following this motivation, these frameworks provide scheduling control through the proxy of resource management. For instance, \cite{mesos} extends opportunistic resource offers and applications may choose to accept and utilize these resources as they require. This mechanism simplifies the application layer logic since the complexity of implementing a full-fledged scheduler is abstracted away, but it limits the flexibility of applications to express compositions of policies which may extend beyond physical resources. For instance, a combination of load-balancing and co-location is challenging with this mechanism since the tasks are bound to each other rather than physical resources.  

% Highlight common problems, and how ExoSched solves them.
Thus, the challenge is to provide clean and flexible interfaces for applications to express heterogenous scheduling requirements without leaking the complexity of scheduling mechanisms. \name{} is a scalable new scheduler design that rethinks resource management to expose high level scheduling policy control. By having the ability to create custom resource labels at runtime, applications can express their scheduling requirements in \name{}, while the core scheduler remains lean and minimalist.  
%\textcolor{red}{TODO: Improve the last paragraph.}

% Simple examples
%todo(romilb): Should this be a general application
% Consider a Reinforcement Learning application that must train an agent on a GPU and then evaluate this agent in a CPU intensive environment simulation. To scale beyond a single machine, this application can leverage a distributed execution framework to launch multiple instances of these agents and environments across the compute resources available to it. However, the placement of these instances is critical to the application's performance - the agent and it's associated environment must be scheduled on the same physical node to avoid network overheads while evaluating the agent. Thus, the challenge is to convey this scheduling requirement to the execution framework.

